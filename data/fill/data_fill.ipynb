{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2b3e3b26",
   "metadata": {},
   "source": [
    "Filling data from `1996 - 2025` of S & P 500 data using `yfinance`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e066ccb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import yfinance as yf\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "from sqlalchemy import create_engine\n",
    "import pandas as pd\n",
    "from datetime import date\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "db_url = os.getenv('DB_URL')\n",
    "db_conn = create_engine(db_url)\n",
    "\n",
    "constituents_table = os.getenv('CONSTIUENTS_TABLE')\n",
    "prices_table = os.getenv('PRICES_TABLE')\n",
    "\n",
    "data_path = os.getenv('DATA_PATH')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c6b1d15",
   "metadata": {},
   "source": [
    "Up to you to have an appropiate database url and table set up. There is:\n",
    "1. `Date`: a `date` column.\n",
    "2. `Close, High, Low, Open`: price columns; I would take 4 decimal points.\n",
    "3. `Volum`: volume columns of type integer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "d8928ad1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ticker_data(ticker: str, start: str, end: str):\n",
    "    df = yf.download(ticker, start=start, end=end, auto_adjust=True)\n",
    "    df.columns = df.columns.map(lambda x: x[0] if isinstance(x, tuple) else x)\n",
    "    df = df.reset_index()\n",
    "    df.index.name = None\n",
    "    df['ticker'] = ticker\n",
    "\n",
    "    for col in ['Close', 'High', 'Low', 'Open']:\n",
    "        df[col] = round(df[col], 4)\n",
    "\n",
    "    df.rename(columns = {\n",
    "        'Date': 'date',\n",
    "        'Close': 'adj_close',\n",
    "        'High': 'high',\n",
    "        'Low': 'low',\n",
    "        'Open': 'open',\n",
    "        'Volume': 'volume',\n",
    "        'ticker': 'ticker'\n",
    "    }, inplace=True)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0654865",
   "metadata": {},
   "outputs": [],
   "source": [
    "# quick e.g of how this works\n",
    "get_ticker_data('AAPL', '2023-01-01', '2025-01-01')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37f2120e",
   "metadata": {},
   "source": [
    "The below code changes a historical database of `S & P 500` constituents I found online (![]())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "486456ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "s_and_p_500_historical = pd.read_csv(f'{data_path}/s_and_p_500.csv')\n",
    "\n",
    "s_and_p_500_historical['tickers'] = s_and_p_500_historical['tickers'].str.split(',')\n",
    "s_and_p_500_historical['date'] = pd.to_datetime(s_and_p_500_historical['date'])\n",
    "\n",
    "s_and_p_500_historical = s_and_p_500_historical.sort_values('date')\n",
    "\n",
    "active = set()\n",
    "records = []\n",
    "\n",
    "for i, row in s_and_p_500_historical.iterrows():\n",
    "    current_date = row['date']\n",
    "    current_tickers = set(row['tickers'])\n",
    "\n",
    "    entered = current_tickers - active\n",
    "    for ticker in entered:\n",
    "        records.append({'ticker': ticker, 'start_date': current_date, 'end_date': None})\n",
    "\n",
    "    left = active - current_tickers\n",
    "    for ticker in left:\n",
    "        for rec in reversed(records):\n",
    "            if rec['ticker'] == ticker and rec['end_date'] is None:\n",
    "                rec['end_date'] = current_date\n",
    "                break\n",
    "\n",
    "    active = current_tickers\n",
    "\n",
    "s_and_p_timeline = pd.DataFrame(records)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "874067f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ticker</th>\n",
       "      <th>start_date</th>\n",
       "      <th>end_date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>HM</td>\n",
       "      <td>1996-01-02</td>\n",
       "      <td>2001-12-17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>DEC</td>\n",
       "      <td>1996-01-02</td>\n",
       "      <td>1998-06-12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>UK</td>\n",
       "      <td>1996-01-02</td>\n",
       "      <td>2001-02-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>SIAL</td>\n",
       "      <td>1996-01-02</td>\n",
       "      <td>2015-11-18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>CNP</td>\n",
       "      <td>1996-01-02</td>\n",
       "      <td>NaT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1225</th>\n",
       "      <td>WSM</td>\n",
       "      <td>2025-04-24</td>\n",
       "      <td>NaT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1226</th>\n",
       "      <td>DASH</td>\n",
       "      <td>2025-04-24</td>\n",
       "      <td>NaT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1227</th>\n",
       "      <td>EXE</td>\n",
       "      <td>2025-04-24</td>\n",
       "      <td>NaT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1228</th>\n",
       "      <td>COIN</td>\n",
       "      <td>2025-05-19</td>\n",
       "      <td>NaT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1229</th>\n",
       "      <td>DDOG</td>\n",
       "      <td>2025-07-09</td>\n",
       "      <td>NaT</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1230 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     ticker start_date   end_date\n",
       "0        HM 1996-01-02 2001-12-17\n",
       "1       DEC 1996-01-02 1998-06-12\n",
       "2        UK 1996-01-02 2001-02-07\n",
       "3      SIAL 1996-01-02 2015-11-18\n",
       "4       CNP 1996-01-02        NaT\n",
       "...     ...        ...        ...\n",
       "1225    WSM 2025-04-24        NaT\n",
       "1226   DASH 2025-04-24        NaT\n",
       "1227    EXE 2025-04-24        NaT\n",
       "1228   COIN 2025-05-19        NaT\n",
       "1229   DDOG 2025-07-09        NaT\n",
       "\n",
       "[1230 rows x 3 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s_and_p_timeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66d12d86",
   "metadata": {},
   "outputs": [],
   "source": [
    "s_and_p_timeline_with_industries = []\n",
    "\n",
    "for i, row in s_and_p_timeline.iterrows():\n",
    "    try:\n",
    "        print(i)\n",
    "        stock_info = yf.Ticker(row['ticker']).info\n",
    "        new_row = row.copy()\n",
    "        if 'sector' in stock_info:\n",
    "            new_row['sector'] = stock_info['sector']\n",
    "        else:\n",
    "            new_row['sector'] = None\n",
    "\n",
    "        s_and_p_timeline_with_industries.append(new_row)\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        new_row = row.copy()\n",
    "        new_row['sector'] = None\n",
    "        s_and_p_timeline_with_industries.append(new_row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d682a34f",
   "metadata": {},
   "outputs": [],
   "source": [
    "constituents = pd.DataFrame(s_and_p_timeline_with_industries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "4cb1698e",
   "metadata": {},
   "outputs": [],
   "source": [
    "constituents['sector'] = constituents['sector'].replace({\n",
    "    'Consumer Defensive': 'Consumer Staples',\n",
    "    'Consumer Cyclical': 'Consumer Discretionary',\n",
    "    'Basic Materials': 'Materials',\n",
    "    'Financial Services': 'Financials'\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42b49917",
   "metadata": {},
   "source": [
    "Write the constituents to a database table if you wish. The `psql` table I used is located in `/data/sql/constituents.sql`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "8c69661f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "230"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "constituents.to_sql('constituents', db_conn, if_exists='append', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3afbf72a",
   "metadata": {},
   "source": [
    "Save constituents to a dump."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "86017832",
   "metadata": {},
   "outputs": [],
   "source": [
    "constituents.to_csv('../dump/constituents.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e810594b",
   "metadata": {},
   "source": [
    "Okay now we fill the actual prices. I worry that I will be blocked pretty quickly from pulling, so I'll add a try catch and loop. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6202df83",
   "metadata": {},
   "outputs": [],
   "source": [
    "aapl_idx = s_and_p_timeline[s_and_p_timeline['ticker'] == 'AAPL'].index[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acd561ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "continue_fill = s_and_p_timeline.iloc[aapl_idx:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e81ab0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "for _, row in continue_fill.iterrows():\n",
    "    if row['start_date'] is pd.NaT:\n",
    "        continue # something has gone wrong here... but constituents table should be fairly good.\n",
    "\n",
    "    # actually we can only pull currently listed stocks\n",
    "\n",
    "    ticker = row['ticker']\n",
    "    start_date = pd.to_datetime(row['start_date']).strftime('%Y-%m-%d')\n",
    "    end_date = row['end_date'] if row['end_date'] is not pd.NaT else date.today().strftime('%Y-%m-%d')\n",
    "\n",
    "    ticker_price_data = get_ticker_data(ticker, start_date, end_date)\n",
    "    ticker_price_data.to_sql(prices_table, db_conn, if_exists='append', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4d531ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# break up into dump files\n",
    "total_rows = pd.read_sql(\"SELECT COUNT(*) FROM adjusted_historical\", db_conn).iloc[0, 0]\n",
    "num_parts = 5\n",
    "chunk_size = total_rows // num_parts\n",
    "\n",
    "for i in range(num_parts):\n",
    "    offset = i * chunk_size\n",
    "    limit = chunk_size if i < num_parts - 1 else total_rows - offset  # ensure we get all remaining rows\n",
    "\n",
    "    query = f\"\"\"\n",
    "        SELECT * FROM adjusted_historical\n",
    "        ORDER BY date, ticker\n",
    "        OFFSET {offset} LIMIT {limit}\n",
    "    \"\"\"\n",
    "\n",
    "    df_chunk = pd.read_sql(query, db_conn)\n",
    "    df_chunk.to_csv(f\"../dump/adjusted_historical_part_{i+1}.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23a54a11",
   "metadata": {},
   "source": [
    "Finally, add the `SPDR` sector ETFs to the prices database. We also add `SPY` as a fallback for any industries that do not have access to sector ETFs for a given period - we expect that this will have weaker betas and more noise thus likely causing more adverse selection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "df3f02f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "sector_etfs = ['XLC', 'XLY', 'XLP', 'XLE', 'XLF', 'XLV', 'XLI', 'XLB', 'XLRE', 'XLK', 'XLU']\n",
    "\n",
    "min_date = pd.read_sql(f'select min(date) from {prices_table}', db_conn)['min'][0].strftime('%Y-%m-%d')\n",
    "max_date = pd.read_sql(f'select max(date) from {prices_table}', db_conn)['max'][0].strftime('%Y-%m-%d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "ea7146e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "******** XLC ********\n",
      "2018-06-20 00:00:00 to 2025-07-17 00:00:00\n",
      "*********************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "******** XLY ********\n",
      "1998-12-23 00:00:00 to 2025-07-17 00:00:00\n",
      "*********************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "******** XLP ********\n",
      "1998-12-23 00:00:00 to 2025-07-17 00:00:00\n",
      "*********************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "******** XLE ********\n",
      "1998-12-23 00:00:00 to 2025-07-17 00:00:00\n",
      "*********************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "******** XLF ********\n",
      "1998-12-23 00:00:00 to 2025-07-17 00:00:00\n",
      "*********************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "******** XLV ********\n",
      "1998-12-23 00:00:00 to 2025-07-17 00:00:00\n",
      "*********************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "******** XLI ********\n",
      "1998-12-23 00:00:00 to 2025-07-17 00:00:00\n",
      "*********************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "******** XLB ********\n",
      "1998-12-23 00:00:00 to 2025-07-17 00:00:00\n",
      "*********************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "******** XLRE ********\n",
      "2015-10-09 00:00:00 to 2025-07-17 00:00:00\n",
      "*********************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "******** XLK ********\n",
      "1998-12-23 00:00:00 to 2025-07-17 00:00:00\n",
      "*********************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "******** XLU ********\n",
      "1998-12-23 00:00:00 to 2025-07-17 00:00:00\n",
      "*********************\n"
     ]
    }
   ],
   "source": [
    "for s in sector_etfs:\n",
    "    sector_data = get_ticker_data(s, min_date, max_date)\n",
    "    print(f'******** {s} ********')\n",
    "    print(f'{sector_data['date'].iloc[1]} to {sector_data['date'].iloc[-1]}')\n",
    "    print('*********************')\n",
    "    sector_data.to_sql(prices_table, db_conn, if_exists='append', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "3a81b3bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "434"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spy = get_ticker_data('SPY', min_date, max_date)\n",
    "spy.to_sql('adjusted_historical', db_conn, if_exists='append', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1acf14a",
   "metadata": {},
   "source": [
    "Now save ETFs as a dump."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6771fcfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "etfs = sector_etfs + ['SPY']\n",
    "sql_etfs = ', '.join(f\"'{e}'\" for e in etfs)\n",
    "\n",
    "etfs_data = pd.read_sql(\n",
    "    f\"\"\"\n",
    "    select * from adjusted_historical where ticker in ({sql_etfs})\n",
    "    \"\"\",\n",
    "    db_conn\n",
    ")\n",
    "\n",
    "etfs_data.to_csv('../dump/adjusted_historical_etfs.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "research",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
